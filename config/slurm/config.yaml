jobs: 100

executor: "cluster-generic"
cluster-generic-submit-cmd: "bash workflows/scripts/submit_slurm.sh \
                              {resources.account} \
                              {rule} \
                              {resources.protein_name} \
                              {resources.partition} \
                              {resources.qos} \
                              {resources.cpus} \
                              {resources.mem_gb}"
                              

software-deployment-method: "conda"

printshellcmds: True
keep-going: True
rerun-incomplete: True
max-jobs-per-second: 2
max-status-checks-per-second: 2
latency-wait: 60
rerun-triggers: "mtime"

default-resources:
  account: users
  partition: short_mdbf
  qos: short_mdbf
  cpus: 1
  mem_gb: 8
  protein_name: 'NA'

set-resources:
  hmmpress:
    partition: mid_mdbf
    qos: mid_mdbf
    cpus: 4
    mem_gb: 40

  make_blasDB:
    partition: mid_mdbf
    qos: mid_mdbf
    cpus: 4
    mem_gb: 50

  psiblast:
    partition: mid_mdbf
    qos: mid_mdbf
    cpus: 4
    mem_gb: 8

  parse_psiblast:
    partition: mid_mdbf
    qos: mid_mdbf
    mem_gb: 50

  mafft_linsi:
    partition: long_mdbf
    qos: long_mdbf
    cpus: 8
    mem_gb: 160

  clipkit:
    partition: mid_mdbf
    qos: mid_mdbf
    cpus: 4
    mem_gb: 16

  modelfinder:
    partition: long_mdbf
    qos: long_mdbf
    cpus: 10
    mem_gb: 20

  parse_modelfinder_output:
    partition: short_mdbf
    qos: short_mdbf
    cpus: 1
    mem_gb: 4

  iqtree:
    partition: longer_mdbf
    qos: longer_mdbf
    cpus: 6
    mem_gb: 20

  hmmscan:
    partition: mid_mdbf
    qos: mid_mdbf
    cpus: 4
    mem_gb: 10

  download_pfamDB:
    partition: mid_mdbf
    qos: mid_mdbf
    mem_gb: 20

  combine_figures:
    mem_gb: 40